import streamlit as st
import google.generativeai as genai
import pandas as pd
from datetime import datetime
import os
import requests

# --- 1. í˜ì´ì§€ ì„¤ì • ---
st.set_page_config(
    page_title="AI Chatbot - 2025 Edition",
    page_icon="ğŸ¤–",
    layout="centered", # ì¤‘ì•™ ì •ë ¬ ë ˆì´ì•„ì›ƒ
    initial_sidebar_state="auto"
)

# --- 2. ì»¤ìŠ¤í…€ CSS ì£¼ì… --- (style.css íŒŒì¼ ë¡œë“œ ë° ì¸ë¼ì¸ CSS)
# style.css íŒŒì¼ì„ ë¡œë“œí•˜ì—¬ ì•± ì „ë°˜ì— ê±¸ì³ ëª¨ë˜í•œ ë‹¤í¬ í…Œë§ˆë¥¼ ì ìš©í•©ë‹ˆë‹¤.
def local_css(file_name):
    # íŒŒì¼ì„ ì—´ê¸° ì „ì— ì¡´ì¬í•˜ëŠ”ì§€ ë¨¼ì € í™•ì¸í•©ë‹ˆë‹¤.
    if os.path.exists(file_name):
        with open(file_name, encoding="utf-8") as f:
            st.markdown(f"<style>{f.read()}</style>", unsafe_allow_html=True)
    else:
        st.warning(f"Warning: CSS file '{file_name}' not found. Styling might be incomplete.")

local_css("style.css")

# ê¸°ì¡´ ì¸ë¼ì¸ CSS ì¤‘ style.cssì— ì—†ëŠ” ë¶€ë¶„ë§Œ ìœ ì§€í•©ë‹ˆë‹¤.
st.markdown("""
<style>
    /* ë©”ì¸ ì»¨í…Œì´ë„ˆì˜ ìµœëŒ€ ë„ˆë¹„ ì¡°ì • */
    .main .block-container {
        max-width: 800px;
        padding-top: 2rem;
        padding-bottom: 2rem;
    }
</style>
""", unsafe_allow_html=True)

# Configure Google Gemini API key from Streamlit secrets
genai.configure(api_key=st.secrets["GOOGLE_API_KEY"])

# --- ì™¸ë¶€ ë„êµ¬(Tools) ì •ì˜ ---
def get_weather(city: str):
    """
    íŠ¹ì • ë„ì‹œì˜ í˜„ì¬ ë‚ ì”¨ ì •ë³´ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
    """
    # ë¬´ë£Œ ë‚ ì”¨ APIì¸ wttr.in ì‚¬ìš©
    try:
        response = requests.get(f"https://wttr.in/{city}?format=j1")
        response.raise_for_status()
        weather_data = response.json()
        current_condition = weather_data['current_condition'][0]
        
        # ëª¨ë¸ì´ ì´í•´í•˜ê¸° ì‰¬ìš´ í˜•íƒœë¡œ ë‚ ì”¨ ì •ë³´ ìš”ì•½
        return (
            f"{city}ì˜ í˜„ì¬ ë‚ ì”¨: "
            f"ë‚ ì”¨ ìƒíƒœ: {current_condition['weatherDesc'][0]['value']}, "
            f"ì˜¨ë„: {current_condition['temp_C']}Â°C, "
            f"ì²´ê° ì˜¨ë„: {current_condition['FeelsLikeC']}Â°C, "
            f"í’ì†: {current_condition['windspeedKmph']}km/h"
        )
    except Exception as e:
        return f"ë‚ ì”¨ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {e}"

# --- ì„±ëŠ¥ ìµœì í™”ë¥¼ ìœ„í•œ ëª¨ë¸ ìºì‹± ---
@st.cache_resource
def load_model():
    """
    ì„¤ëª…: AI ëª¨ë¸(GenerativeModel)ì€ ë¡œë“œí•˜ëŠ” ë° ì‹œê°„ì´ ê±¸ë¦¬ëŠ” ë¬´ê±°ìš´ ê°ì²´ì…ë‹ˆë‹¤.
    @st.cache_resource ë°ì½”ë ˆì´í„°ëŠ” ì´ í•¨ìˆ˜ê°€ ì•± ì„¸ì…˜ì—ì„œ ë”± í•œ ë²ˆë§Œ ì‹¤í–‰ë˜ë„ë¡ ë³´ì¥í•©ë‹ˆë‹¤.
    ì´í›„ í•¨ìˆ˜ í˜¸ì¶œ ì‹œì—ëŠ” ìƒˆë¡œ ëª¨ë¸ì„ ë¡œë“œí•˜ëŠ” ëŒ€ì‹ , ë©”ëª¨ë¦¬ì— ì €ì¥ëœ ê¸°ì¡´ ê°ì²´ë¥¼ ì¦‰ì‹œ ë°˜í™˜í•©ë‹ˆë‹¤.
    ì´ì : ì‚¬ìš©ìê°€ ë©”ì‹œì§€ë¥¼ ë³´ë‚¼ ë•Œë§ˆë‹¤ ëª¨ë¸ì„ ìƒˆë¡œ ë¡œë“œí•˜ëŠ” ë¹„íš¨ìœ¨ì„ ì—†ì• ê³  ì•±ì˜ ë°˜ì‘ ì†ë„ë¥¼ í¬ê²Œ í–¥ìƒì‹œí‚µë‹ˆë‹¤.
    """
    # í•¨ìˆ˜ í˜¸ì¶œ ê¸°ëŠ¥ì„ ì‚¬ìš©í•˜ê¸° ìœ„í•´ tools ë§¤ê°œë³€ìˆ˜ì™€ í•¨ê»˜ ëª¨ë¸ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
    model = genai.GenerativeModel(
        model_name='gemini-2.5-pro', tools=[get_weather])
    return model

# --- í”¼ë“œë°± ì €ì¥ì„ ìœ„í•œ í•¨ìˆ˜ ---
def save_feedback(message_index, rating, feedback_text=""):
    """
    ì‚¬ìš©ì í”¼ë“œë°±ì„ CSV íŒŒì¼ì— ì €ì¥í•©ë‹ˆë‹¤.
    """
    # í”¼ë“œë°±ì˜ ëŒ€ìƒì´ ë˜ëŠ” ì‚¬ìš©ì ì§ˆë¬¸ê³¼ AI ë‹µë³€ì„ ì°¾ìŠµë‹ˆë‹¤.
    # AI ë‹µë³€ì€ message_indexì— ìˆê³ , ì‚¬ìš©ì ì§ˆë¬¸ì€ ê·¸ ë°”ë¡œ ì•ì— ìˆìŠµë‹ˆë‹¤.
    if message_index > 0:
        user_question = st.session_state.messages[message_index - 1]['content']
    else:
        user_question = "" # ëŒ€í™” ì‹œì‘ ë©”ì‹œì§€ì— ëŒ€í•œ í”¼ë“œë°±ì¼ ê²½ìš°

    ai_answer = st.session_state.messages[message_index]['content']

    feedback_data = {
        "timestamp": [datetime.now().strftime("%Y-%m-%d %H:%M:%S")],
        "user_question": [user_question],
        "ai_answer": [ai_answer],
        "rating": [rating],
        "feedback_text": [feedback_text]
    }
    df = pd.DataFrame(feedback_data)

    # íŒŒì¼ì´ ì¡´ì¬í•˜ë©´ ê¸°ì¡´ ë‚´ìš©ì— ì¶”ê°€í•˜ê³ , ì—†ìœ¼ë©´ ìƒˆë¡œ ë§Œë“­ë‹ˆë‹¤.
    if os.path.exists("feedback.csv"):
        df.to_csv("feedback.csv", mode='a', header=False, index=False, encoding='utf-8-sig')
    else:
        df.to_csv("feedback.csv", mode='w', header=True, index=False, encoding='utf-8-sig')


# Function to translate role from 'assistant' to 'model'
def translate_role_for_gemini(role):
    if role == "assistant":
        return "model"
    else:
        return role

# --- ì‚¬ì´ë“œë°” êµ¬ì„± (UX ê°œì„ ) ---
with st.sidebar:
    st.header("ëŒ€í™” ê´€ë¦¬")
    if st.button("ìƒˆ ëŒ€í™” ì‹œì‘", use_container_width=True):
        st.session_state.messages = [] # ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™”
        st.rerun()
    
    # ëŒ€í™” ë‚´ìš© ë‹¤ìš´ë¡œë“œ ê¸°ëŠ¥
    if st.session_state.get("messages"): # ë©”ì‹œì§€ê°€ ìˆì„ ë•Œë§Œ ë²„íŠ¼ í‘œì‹œ
        # ëŒ€í™” ê¸°ë¡ì„ í•˜ë‚˜ì˜ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
        chat_history = "\n\n".join(
            f"**{m['role'].capitalize()}**: {m['content']}" 
            for m in st.session_state.messages
        )
        st.download_button(
            label="ëŒ€í™” ë‚´ìš© ë‹¤ìš´ë¡œë“œ",
            data=chat_history.encode('utf-8'),
            file_name="chatbot_history.txt",
            mime="text/plain",
            use_container_width=True
        )

# --- 3. UI ë ˆì´ì•„ì›ƒ êµ¬ì„± ---
st.title("ğŸ¤– AI Chatbot (2025 Edition)")
st.caption("ì„œìš¸êµëŒ€ ì•±í”„ë¡œê·¸ë˜ë° ìˆ˜ì—… ì „ìš© ì±—ë´‡")

# ëŒ€í™” ê¸°ë¡ì„ ë‹´ì„ ì»¨í…Œì´ë„ˆ
chat_container = st.container(height=500)

with chat_container:
    if "messages" not in st.session_state:
        # Gemini APIëŠ” 'assistant' ëŒ€ì‹  'model' ì—­í• ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
        st.session_state.messages = [{"role": "model", "content": "ì•ˆë…•í•˜ì„¸ìš”! ì €ëŠ” 2025ë…„ 11ì›” ë²„ì „ì˜ AI ì±—ë´‡ì…ë‹ˆë‹¤. ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”."}]

    for idx, message in enumerate(st.session_state.messages):
        with st.chat_message(translate_role_for_gemini(message["role"])):
            st.markdown(message["content"])

            # AI ë‹µë³€(assistant ì—­í• )ì—ë§Œ í”¼ë“œë°± ë²„íŠ¼ ì¶”ê°€
            if message["role"] == "assistant":
                feedback_key_base = f"feedback_{idx}"
                
                col1, col2, _ = st.columns([1, 1, 8])
                with col1:
                    if st.button("ğŸ‘", key=f"{feedback_key_base}_like"):
                        save_feedback(idx, "ğŸ‘ ì¢‹ì•˜ì–´ìš”")
                        st.toast("í”¼ë“œë°±ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤. ê°ì‚¬í•©ë‹ˆë‹¤! ğŸ˜Š")
                with col2:
                    if st.button("ğŸ‘", key=f"{feedback_key_base}_dislike"):
                        # 'ì•„ì‰¬ì›Œìš”' ë²„íŠ¼ì„ ëˆ„ë¥´ë©´ í”¼ë“œë°± ì…ë ¥ì°½ì„ í‘œì‹œí•˜ê¸° ìœ„í•œ ìƒíƒœ ì €ì¥
                        st.session_state[f"show_feedback_input_{idx}"] = True
                
                # 'ì•„ì‰¬ì›Œìš”'ê°€ ì„ íƒëœ ê²½ìš°, í…ìŠ¤íŠ¸ ì…ë ¥ì°½ í‘œì‹œ
                if st.session_state.get(f"show_feedback_input_{idx}"):
                    feedback_text = st.text_area("ì–´ë–¤ ì ì´ ì•„ì‰¬ì› ëŠ”ì§€ ì•Œë ¤ì£¼ì‹¤ ìˆ˜ ìˆë‚˜ìš”?", key=f"{feedback_key_base}_text")
                    if st.button("í”¼ë“œë°± ì œì¶œ", key=f"{feedback_key_base}_submit"):
                        save_feedback(idx, "ğŸ‘ ì•„ì‰¬ì›Œìš”", feedback_text)
                        st.toast("ì†Œì¤‘í•œ í”¼ë“œë°±ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤. ë” ë°œì „í•˜ëŠ” AIê°€ ë˜ê² ìŠµë‹ˆë‹¤! ğŸ™‡â€â™‚ï¸")
                        # ì œì¶œ í›„ ì…ë ¥ì°½ ìˆ¨ê¸°ê¸°
                        st.session_state[f"show_feedback_input_{idx}"] = False
                        st.rerun()

# --- 4. ì±—ë´‡ ë¡œì§ ---
if prompt := st.chat_input("ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”..."):
    # ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ê¸°ë¡í•˜ê³  í™”ë©´ì— í‘œì‹œ
    st.session_state.messages.append({"role": "user", "content": prompt})
    with chat_container:
        with st.chat_message("user"):
            st.markdown(prompt)

    # AI ì‘ë‹µì„ ìƒì„±í•˜ê³  í™”ë©´ì— í‘œì‹œ
    with chat_container:
        with st.chat_message("assistant"):
            message_placeholder = st.empty()
            full_response = ""

            # ìºì‹œëœ ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤. ì•± ì‹¤í–‰ í›„ ìµœì´ˆ í•œ ë²ˆë§Œ ë¡œë“œë˜ê³  ì´í›„ì—ëŠ” ì¦‰ì‹œ ë°˜í™˜ë©ë‹ˆë‹¤.
            model = load_model()

            gemini_messages = [
                {"role": translate_role_for_gemini(m["role"]), "parts": [m["content"]]}
                for m in st.session_state.messages
            ]

            # --- ì‹¤ì‹œê°„ íƒ€ì´í•‘ íš¨ê³¼ ë° í•¨ìˆ˜ í˜¸ì¶œ ë¡œì§ ---
            response_stream = model.generate_content(gemini_messages, stream=True)
            
            function_call_info = None
            
            # 1ì°¨ ìŠ¤íŠ¸ë¦¬ë°: í…ìŠ¤íŠ¸ë¥¼ ì¶œë ¥í•˜ê±°ë‚˜ í•¨ìˆ˜ í˜¸ì¶œ ì •ë³´ë¥¼ ì°¾ìŠµë‹ˆë‹¤.
            for chunk in response_stream:
                # ëª¨ë¸ì´ í•¨ìˆ˜ í˜¸ì¶œì„ ìš”ì²­í–ˆëŠ”ì§€ í™•ì¸
                if (part := chunk.parts[0]).function_call:
                    function_call_info = part.function_call
                    break # í•¨ìˆ˜ í˜¸ì¶œì´ ê°ì§€ë˜ë©´ í…ìŠ¤íŠ¸ ì¶œë ¥ì„ ë©ˆì¶¥ë‹ˆë‹¤.
                # í…ìŠ¤íŠ¸ê°€ ìˆìœ¼ë©´ ì‹¤ì‹œê°„ìœ¼ë¡œ ì¶œë ¥
                if chunk.text:
                    full_response += chunk.text
                    message_placeholder.markdown(full_response + "â–Œ")
            
            # í•¨ìˆ˜ í˜¸ì¶œì´ ê°ì§€ëœ ê²½ìš°, í›„ì† ì²˜ë¦¬ë¥¼ í•©ë‹ˆë‹¤.
            if function_call_info:
                function_name = function_call_info.name
                if function_name == "get_weather":
                    city = function_call_info.args['city']
                    message_placeholder.markdown(f"`{city}`ì˜ ë‚ ì”¨ë¥¼ ê²€ìƒ‰ ì¤‘ì…ë‹ˆë‹¤... ğŸ›°ï¸")
                    function_response = get_weather(city=city)

                    # 2ì°¨ ìŠ¤íŠ¸ë¦¬ë°: í•¨ìˆ˜ì˜ ì‹¤í–‰ ê²°ê³¼ë¥¼ ëª¨ë¸ì—ê²Œ ë‹¤ì‹œ ì „ë‹¬í•˜ì—¬ ìµœì¢… ë‹µë³€ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ë°›ìŠµë‹ˆë‹¤.
                    final_response_stream = model.generate_content([
                        *gemini_messages,
                        response_stream.candidates[0].content, # ëª¨ë¸ì˜ í•¨ìˆ˜ í˜¸ì¶œ ìš”ì²­
                        {'role': 'tool', 'parts': [{'function_response': {'name': function_name, 'response': {'result': function_response}}}]}
                    ], stream=True)
                    
                    full_response = "" # ìµœì¢… ë‹µë³€ì„ ìœ„í•´ ì´ˆê¸°í™”
                    for chunk in final_response_stream:
                        if chunk.text:
                            full_response += chunk.text
                            message_placeholder.markdown(full_response + "â–Œ")
            
            message_placeholder.markdown(full_response) # ìµœì¢… ì™„ì„±ëœ ë‹µë³€ í‘œì‹œ
    
    st.session_state.messages.append({"role": "assistant", "content": full_response})
    st.rerun() # ì±„íŒ… ì…ë ¥ í›„ ìŠ¤í¬ë¡¤ì„ ë§¨ ì•„ë˜ë¡œ ìœ ì§€í•˜ê¸° ìœ„í•´ ìƒˆë¡œê³ ì¹¨