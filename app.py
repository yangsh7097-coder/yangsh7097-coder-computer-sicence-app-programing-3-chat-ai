import streamlit as st
import feedparser
import requests
from bs4 import BeautifulSoup
from transformers import pipeline

# --- í˜ì´ì§€ ê¸°ë³¸ ì„¤ì • ---
st.set_page_config(
    page_title="ì˜¤ëŠ˜ì˜ ê²½ì œ ë‰´ìŠ¤ ìš”ì•½",
    page_icon="ğŸ“°",
    layout="wide"
)

# --- ëª¨ë¸ ë¡œë”© ---
# @st.cache_resource ë°ì½”ë ˆì´í„°ëŠ” ë¬´ê±°ìš´ ëª¨ë¸ì„ í•œë²ˆë§Œ ë¡œë”©í•˜ê³  ìºì‹œì— ì €ì¥í•˜ì—¬
# ì•±ì˜ ì„±ëŠ¥ì„ í¬ê²Œ í–¥ìƒì‹œí‚µë‹ˆë‹¤.
@st.cache_resource
def load_summarizer():
    """í…ìŠ¤íŠ¸ ìš”ì•½ ëª¨ë¸ì„ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜"""
    # í•œêµ­ì–´ ìš”ì•½ì— íŠ¹í™”ëœ ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
    summarizer = pipeline("summarization", model="gogamza/kobart-summarization")
    return summarizer

summarizer = load_summarizer()

# --- í•¨ìˆ˜ ì •ì˜ ---

@st.cache_data(ttl=3600) # 1ì‹œê°„ ë™ì•ˆ ë‰´ìŠ¤ ëª©ë¡ ìºì‹œ
def get_news_feed(url):
    """RSS í”¼ë“œì—ì„œ ë‰´ìŠ¤ ëª©ë¡ì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜"""
    try:
        # feedparserì˜ ë‚´ì¥ ë¡œì§ ëŒ€ì‹  requestsë¥¼ ì‚¬ìš©í•´ ì•ˆì •ì ìœ¼ë¡œ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36'}
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        # ê°€ì ¸ì˜¨ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ feedparserë¡œ íŒŒì‹±í•©ë‹ˆë‹¤.
        feed = feedparser.parse(response.content)
        return feed.entries
    except requests.exceptions.RequestException as e:
        st.error(f"RSS í”¼ë“œë¥¼ ê°€ì ¸ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        return [] # ì‹¤íŒ¨ ì‹œ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜

def get_article_text(url, source_name):
    """ë‰´ìŠ¤ ê¸°ì‚¬ URLì—ì„œ ë³¸ë¬¸ í…ìŠ¤íŠ¸ë¥¼ ìŠ¤í¬ë ˆì´í•‘í•˜ëŠ” í•¨ìˆ˜"""
    try:
        response = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'})
        response.raise_for_status() # HTTP ì˜¤ë¥˜ê°€ ë°œìƒí•˜ë©´ ì˜ˆì™¸ë¥¼ ë°œìƒì‹œí‚´
        soup = BeautifulSoup(response.text, "html.parser")

        content = None
        # ë‰´ìŠ¤ ì¶œì²˜ì— ë”°ë¼ ë‹¤ë¥¸ íŒŒì‹± ê·œì¹™ ì ìš©
        if "í•œêµ­ê²½ì œ" in source_name:
            content = soup.find("div", id="article-body")
        elif "ë§¤ì¼ê²½ì œ" in source_name:
            content = soup.find("div", id="article_body")
            # ë§¤ì¼ê²½ì œì˜ ê²½ìš°, ë³¸ë¬¸(<p> íƒœê·¸)ë§Œ ì •í™•íˆ ì¶”ì¶œí•˜ì—¬ ì¬êµ¬ì„±í•©ë‹ˆë‹¤.
            if content:
                # ë³¸ë¬¸ ì˜ì—­ì—ì„œ ëª¨ë“  p íƒœê·¸ë¥¼ ì°¾ì•„ í…ìŠ¤íŠ¸ë¥¼ í•©ì¹©ë‹ˆë‹¤.
                # ì´ë ‡ê²Œ í•˜ë©´ ê´€ë ¨ê¸°ì‚¬, ê¸°ìì •ë³´ ë“± ë¶ˆí•„ìš”í•œ divê°€ ìë™ìœ¼ë¡œ ì œì™¸ë©ë‹ˆë‹¤.
                paragraphs = content.find_all('p')
                article_text = '\n'.join(p.get_text(strip=True) for p in paragraphs)
                return article_text

        if content:
            # ë¶ˆí•„ìš”í•œ íƒœê·¸(ê´‘ê³ , ê´€ë ¨ê¸°ì‚¬ ë§í¬ ë“±) ì œê±°
            for tag in content.find_all(['script', 'iframe', 'style', 'figure']):
                tag.decompose()
            return content.get_text(separator="\n", strip=True)

        return "ê¸°ì‚¬ ë³¸ë¬¸ì„ ê°€ì ¸ì˜¤ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ì‚¬ì´íŠ¸ êµ¬ì¡°ê°€ ë³€ê²½ë˜ì—ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
    except requests.exceptions.RequestException as e:
        return f"ê¸°ì‚¬ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"

def summarize_text(text):
    """ì…ë ¥ëœ í…ìŠ¤íŠ¸ë¥¼ ìš”ì•½í•˜ëŠ” í•¨ìˆ˜"""
    if len(text.strip()) < 200: # ê³µë°±ì„ ì œì™¸í•œ ê¸¸ì´ê°€ 200ì ë¯¸ë§Œì´ë©´ ìš”ì•½í•˜ì§€ ì•ŠìŒ
        return "ìš”ì•½í•˜ê¸°ì—ëŠ” ê¸°ì‚¬ ë‚´ìš©ì´ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤."
    
    # ëª¨ë¸ì´ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ìµœëŒ€ ê¸¸ì´ì— ë§ì¶° í…ìŠ¤íŠ¸ë¥¼ ìë¦…ë‹ˆë‹¤.
    max_chunk_size = 1024
    if len(text) > max_chunk_size:
        text = text[:max_chunk_size]

    try:
        summary = summarizer(text, max_length=150, min_length=30, do_sample=False)
        return summary[0]['summary_text']
    except Exception as e:
        return f"ìš”ì•½ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"

# --- UI êµ¬ì„± ---
st.title("ğŸ“° ì˜¤ëŠ˜ì˜ ê²½ì œ ë‰´ìŠ¤ ìš”ì•½")
st.write("RSS í”¼ë“œë¥¼ í†µí•´ ìµœì‹  ê²½ì œ ë‰´ìŠ¤ë¥¼ ë¶ˆëŸ¬ì™€ AIë¡œ ìš”ì•½í•©ë‹ˆë‹¤.")

st.markdown("---")

# ë‰´ìŠ¤ ì†ŒìŠ¤ ì„ íƒ
RSS_FEEDS = {
    "í•œêµ­ê²½ì œ (ì£¼ìš”ë‰´ìŠ¤)": "https://www.hankyung.com/rss/major.xml",
    "ë§¤ì¼ê²½ì œ (ì „ì²´ê¸°ì‚¬)": "https://www.mk.co.kr/rss/all/30000001/",
}

selected_feed_name = st.sidebar.selectbox("ë‰´ìŠ¤ ì¶œì²˜ë¥¼ ì„ íƒí•˜ì„¸ìš”:", list(RSS_FEEDS.keys()))
rss_url = RSS_FEEDS[selected_feed_name]

news_list = get_news_feed(rss_url)

if not news_list:
    st.error("ë‰´ìŠ¤ í”¼ë“œë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
else:
    # ë‰´ìŠ¤ ê¸°ì‚¬ ì œëª© ëª©ë¡ ìƒì„±
    news_titles = [news.title for news in news_list]
    selected_title = st.selectbox("ìš”ì•½í•  ë‰´ìŠ¤ë¥¼ ì„ íƒí•˜ì„¸ìš”:", news_titles)
    
    # ì„ íƒëœ ê¸°ì‚¬ ì •ë³´ ì°¾ê¸°
    selected_news = None
    for news in news_list:
        if news.title == selected_title:
            selected_news = news
            break

    if selected_news:
        st.subheader(selected_news.title)
        st.write(f"**ì¶œì²˜:** {selected_feed_name} | **ë°œí–‰ì¼:** {selected_news.get('published', 'N/A')}")
        st.markdown(f"[ì›ë¬¸ ê¸°ì‚¬ ì½ê¸°]({selected_news.link})", unsafe_allow_html=True)

        if st.button("ì´ ê¸°ì‚¬ ìš”ì•½í•˜ê¸°"):
            with st.spinner("ê¸°ì‚¬ ë³¸ë¬¸ì„ ê°€ì ¸ì™€ AIë¡œ ìš”ì•½ ì¤‘ì…ë‹ˆë‹¤... ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”."):
                article_text = get_article_text(selected_news.link, selected_feed_name)
                summary = summarize_text(article_text)
                
                st.subheader("ğŸ¤– AI ìš”ì•½")
                st.write(summary)
